{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1A-RR1Ep-wU",
        "outputId": "82233ce3-16e1-4e85-9d36-db3dd89ec083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.7282235026359558\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('abalone.data.csv')\n",
        "\n",
        "categorical_mask = data.dtypes == object\n",
        "categorical_cols = data.columns[categorical_mask].tolist()\n",
        "numerical_cols = data.columns[~categorical_mask].tolist()\n",
        "\n",
        "encoders = {}\n",
        "data_encoded = data.copy()\n",
        "for col in categorical_cols:\n",
        "   le = LabelEncoder()\n",
        "   data_encoded[col] = le.fit_transform(data[col])\n",
        "   encoders[col] = (le, OneHotEncoder())\n",
        "   encoded_cols = encoders[col][1].fit_transform(data_encoded[col].values.reshape(-1, 1)).toarray()\n",
        "   data_encoded = data_encoded.drop(col, axis=1)\n",
        "   data_encoded = pd.concat([data_encoded, pd.DataFrame(encoded_cols, columns=[f\"{col}_{i}\" for i in range(encoded_cols.shape[1])])], axis=1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_encoded[numerical_cols] = scaler.fit_transform(data_encoded[numerical_cols])\n",
        "\n",
        "X = data_encoded.drop('Rings', axis=1)\n",
        "y = data_encoded['Rings']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_tensor = torch.from_numpy(X_train.values).float()\n",
        "y_train_tensor = torch.from_numpy(y_train.values).float().unsqueeze(1)\n",
        "X_test_tensor = torch.from_numpy(X_test.values).float()\n",
        "y_test_tensor = torch.from_numpy(y_test.values).float().unsqueeze(1)\n",
        "\n",
        "class AbaloneModel(nn.Module):\n",
        "   def __init__(self, input_size, hidden_sizes, output_size):\n",
        "       super(AbaloneModel, self).__init__()\n",
        "       self.layers = nn.ModuleList([nn.Linear(input_size, hidden_sizes[0])])\n",
        "       self.layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)])\n",
        "       self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "       self.relu = nn.ReLU()\n",
        "\n",
        "   def forward(self, x):\n",
        "       for i, layer in enumerate(self.layers[:-1]):\n",
        "           x = self.relu(layer(x))\n",
        "       x = self.layers[-1](x)\n",
        "       return x\n",
        "\n",
        "input_size = X_train_tensor.shape[1]\n",
        "hidden_sizes = [64, 32]\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "model = AbaloneModel(input_size, hidden_sizes, output_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   model.train()\n",
        "   optimizer.zero_grad()\n",
        "   outputs = model(X_train_tensor)\n",
        "   loss = criterion(outputs, y_train_tensor)\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "   model.eval()\n",
        "   with torch.no_grad():\n",
        "       outputs = model(X_test)\n",
        "       mse = nn.MSELoss()\n",
        "       loss = mse(outputs, y_test)\n",
        "   return loss.item()\n",
        "\n",
        "test_loss = evaluate(model, X_test_tensor, y_test_tensor)\n",
        "print(f\"Test MSE: {test_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to train and evaluate the model with different hyperparameters\n",
        "def train_and_evaluate(learning_rate, batch_size, hidden_sizes, num_epochs=100):\n",
        "    model = AbaloneModel(input_size, hidden_sizes, output_size)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i in range(0, X_train_tensor.shape[0], batch_size):\n",
        "            inputs = X_train_tensor[i:i+batch_size]\n",
        "            labels = y_train_tensor[i:i+batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    test_loss = evaluate(model, X_test_tensor, y_test_tensor)\n",
        "    return test_loss\n",
        "\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "hidden_sizes_list = [[32, 16], [64, 32], [128, 64], [256, 128]]\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for bs in batch_sizes:\n",
        "        for hs in hidden_sizes_list:\n",
        "            test_loss = train_and_evaluate(lr, bs, hs)\n",
        "            results.append({\"learning_rate\": lr, \"batch_size\": bs, \"hidden_sizes\": hs, \"test_mse\": test_loss})\n",
        "\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPDrs_CywRUw",
        "outputId": "2f43b563-d535-43a2-8690-917841cd74f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    learning_rate  batch_size hidden_sizes  test_mse\n",
            "0            0.01          16     [32, 16]  0.467627\n",
            "1            0.01          16     [64, 32]  0.474029\n",
            "2            0.01          16    [128, 64]  0.474439\n",
            "3            0.01          16   [256, 128]  0.474443\n",
            "4            0.01          32     [32, 16]  0.483013\n",
            "..            ...         ...          ...       ...\n",
            "59           0.50          64   [256, 128]  0.709121\n",
            "60           0.50         128     [32, 16]  0.516233\n",
            "61           0.50         128     [64, 32]  0.475749\n",
            "62           0.50         128    [128, 64]  1.279263\n",
            "63           0.50         128   [256, 128]  1.279401\n",
            "\n",
            "[64 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding more layers to the model\n",
        "class AbaloneModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(AbaloneModel, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "model = AbaloneModel(input_size, [128, 64, 32], output_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "test_loss = evaluate(model, X_test_tensor, y_test_tensor)\n",
        "print(f\"Test MSE (Adagrad): {test_loss}\")\n",
        "\n",
        "sgd_optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    sgd_optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    sgd_optimizer.step()\n",
        "\n",
        "sgd_test_loss = evaluate(model, X_test_tensor, y_test_tensor)\n",
        "print(f\"Test MSE (SGD): {sgd_test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fO-I7t5wsoW",
        "outputId": "caecff86-b863-47e2-f24e-fdda56ce8787"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE (Adagrad): 0.4400298595428467\n",
            "Test MSE (SGD): 0.43521052598953247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model with 10-15 hidden layers and Sigmoid activation\n",
        "class AbaloneModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(AbaloneModel, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(nn.Sigmoid())\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "hidden_sizes = [64] * 10  # or [32] * 15\n",
        "model = AbaloneModel(input_size, hidden_sizes, output_size)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "test_loss = evaluate(model, X_test_tensor, y_test_tensor)\n",
        "print(f\"Test MSE (Sigmoid, 10-15 hidden layers): {test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kog_ud_sw47x",
        "outputId": "e6862241-8089-40d2-ff92-4f68f3da71fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE (Sigmoid, 10-15 hidden layers): 1.0418965816497803\n"
          ]
        }
      ]
    }
  ]
}